# Fine-tuning

## Project: Fine-Tuning LLaMA3-8B for my Honeypot

In this project, I am fine-tuning the **LLaMA3-8B** model. Since I am working with a single Kaggle GPU, I am using Meta's **LoRA** (Low-Rank Adaptation) method to optimize training efficiency. 

The dataset I am fine-tuning the model with consists of commands and their respective outputs. The goal is to fine-tune the LLM to produce more realistic and context-aware outputs for my LLM-based honeypot project, if you want more detail about this work, you can find my entire project [here](https://github.com/Salakche/LLM_honeypot).
